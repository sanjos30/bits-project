sandeepjoshi@sjmac bits_project % python improved_model_trainer.py
ðŸŽ¯ IMPROVED MODEL TRAINING FOR BETTER RESPONSES
======================================================================
ðŸ“± Device: cpu
ðŸ“Š Creating high-quality financial training data...
âœ… Loaded 1,000 users and 1,000,000 transactions
âœ… Created 1600 high-quality training examples

ðŸ¤– Loading model and tokenizer...
ðŸ”§ Setting up optimal LoRA configuration...
trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8607

ðŸ“‹ Preparing enhanced dataset...
âœ… Training examples: 1440
âœ… Evaluation examples: 160

ðŸš€ Starting enhanced training...
  0%|                                                                                        | 0/1440 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
{'loss': 4.007, 'grad_norm': 0.9928969740867615, 'learning_rate': 2.45e-05, 'epoch': 0.28}                            
{'loss': 3.4244, 'grad_norm': 1.2769895792007446, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.56}             
{'loss': 2.2511, 'grad_norm': 1.8642628192901611, 'learning_rate': 4.817164179104478e-05, 'epoch': 0.83}              
{'loss': 1.4636, 'grad_norm': 3.7889952659606934, 'learning_rate': 4.6305970149253736e-05, 'epoch': 1.11}             
{'eval_loss': 1.0298562049865723, 'eval_runtime': 69.1371, 'eval_samples_per_second': 2.314, 'eval_steps_per_second': 2.314, 'epoch': 1.11}                                                                                                 
{'loss': 1.097, 'grad_norm': 2.0420081615448, 'learning_rate': 4.4440298507462694e-05, 'epoch': 1.39}                 
{'loss': 0.9479, 'grad_norm': 2.303584575653076, 'learning_rate': 4.2574626865671645e-05, 'epoch': 1.67}              
{'loss': 0.8846, 'grad_norm': 1.2551738023757935, 'learning_rate': 4.07089552238806e-05, 'epoch': 1.94}               
{'loss': 0.8278, 'grad_norm': 1.4403079748153687, 'learning_rate': 3.8843283582089554e-05, 'epoch': 2.22}             
{'eval_loss': 0.7400363683700562, 'eval_runtime': 67.3899, 'eval_samples_per_second': 2.374, 'eval_steps_per_second': 2.374, 'epoch': 2.22}                                                                                                 
{'loss': 0.8334, 'grad_norm': 1.3146929740905762, 'learning_rate': 3.6977611940298505e-05, 'epoch': 2.5}              
{'loss': 0.8186, 'grad_norm': 1.5685909986495972, 'learning_rate': 3.511194029850746e-05, 'epoch': 2.78}              
{'loss': 0.8006, 'grad_norm': 1.42664635181427, 'learning_rate': 3.3246268656716414e-05, 'epoch': 3.06}               
{'loss': 0.7925, 'grad_norm': 1.423795223236084, 'learning_rate': 3.138059701492537e-05, 'epoch': 3.33}               
{'eval_loss': 0.7215906977653503, 'eval_runtime': 79.0623, 'eval_samples_per_second': 2.024, 'eval_steps_per_second': 2.024, 'epoch': 3.33}                                                                                                 
{'loss': 0.7755, 'grad_norm': 1.0088506937026978, 'learning_rate': 2.9514925373134326e-05, 'epoch': 3.61}             
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 657/1440 [1:01:03<1:12:25,  5.55s/it]{'loss': 0.7883, 'grad_norm': 3.22540545463562, 'learning_rate': 2.7649253731343284e-05, 'epoch': 3.89}
{'loss': 0.7781, 'grad_norm': 1.128212332725525, 'learning_rate': 2.578358208955224e-05, 'epoch': 4.17}
{'loss': 0.7743, 'grad_norm': 0.8917509913444519, 'learning_rate': 2.3917910447761197e-05, 'epoch': 4.44}
{'eval_loss': 0.710146427154541, 'eval_runtime': 70.7751, 'eval_samples_per_second': 2.261, 'eval_steps_per_second': 2.261, 'epoch': 4.44}                                                            
{'loss': 0.7551, 'grad_norm': 1.2300933599472046, 'learning_rate': 2.2052238805970148e-05, 'epoch': 4.72}        
{'loss': 0.7841, 'grad_norm': 1.2562754154205322, 'learning_rate': 2.0186567164179106e-05, 'epoch': 5.0}         
{'loss': 0.7537, 'grad_norm': 1.0651737451553345, 'learning_rate': 1.832089552238806e-05, 'epoch': 5.28}         
{'loss': 0.7591, 'grad_norm': 0.9566937685012817, 'learning_rate': 1.6455223880597015e-05, 'epoch': 5.56}        
{'eval_loss': 0.7056930661201477, 'eval_runtime': 69.6639, 'eval_samples_per_second': 2.297, 'eval_steps_per_second': 2.297, 'epoch': 5.56}                                                                                       
{'loss': 0.7598, 'grad_norm': 1.114558219909668, 'learning_rate': 1.458955223880597e-05, 'epoch': 5.83}          
{'loss': 0.7729, 'grad_norm': 0.8139269948005676, 'learning_rate': 1.2723880597014926e-05, 'epoch': 6.11}        
{'loss': 0.768, 'grad_norm': 1.2576394081115723, 'learning_rate': 1.085820895522388e-05, 'epoch': 6.39}          
{'loss': 0.7512, 'grad_norm': 0.8992570042610168, 'learning_rate': 8.992537313432836e-06, 'epoch': 6.67}         
{'eval_loss': 0.6981992721557617, 'eval_runtime': 94.4787, 'eval_samples_per_second': 1.694, 'eval_steps_per_second': 1.694, 'epoch': 6.67}                                                                                       
{'loss': 0.7606, 'grad_norm': 1.0081307888031006, 'learning_rate': 7.126865671641792e-06, 'epoch': 6.94}         
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 1263/1440 [2:03:10<30:56, 10.49s/it]{'loss': 0.7412, 'grad_norm': 0.93353670835495, 'learning_rate': 5.261194029850746e-06, 'epoch': 7.22}    
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1333/1440 [2:15:54<26:13, 14.71s/it]{'loss': 0.7434, 'grad_norm': 1.059199333190918, 'learning_rate': 3.3955223880597013e-06, 'epoch': 7.5}
{'loss': 0.7449, 'grad_norm': 1.062599539756775, 'learning_rate': 1.5298507462686568e-06, 'epoch': 7.78}
{'eval_loss': 0.692415177822113, 'eval_runtime': 76.056, 'eval_samples_per_second': 2.104, 'eval_steps_per_second': 2.104, 'epoch': 7.78}         
{'train_runtime': 8906.451, 'train_samples_per_second': 1.293, 'train_steps_per_second': 0.162, 'train_loss': 1.0743184619479709, 'epoch': 8.0}   
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1440/1440 [2:28:26<00:00,  6.19s/it]
âœ… Enhanced training completed in 2:28:26.590368
ðŸ’¾ Improved model saved to: ./models/improved_financial_lora

ðŸ§ª Testing improved model...
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
ðŸ”¹ Q: How much did I spend on groceries?
ðŸ¤– A: You spent â‚¹48,971 in total across all categories. Your highest spending was salary at â¸19094,749 from education (âƒˆ21.,148%). Consider reducing expenses to 25% of income. Recommended

ðŸ”¹ Q: What's my investment recommendation?
ðŸ¤– A: Based on your age (37), I recommend 55% equity and 37% debt. Consider investing â‚¹5,913 monthly. Recommended SIP: Keep improving! â€” Tristan (@tracy_tristan) November 27th,

ðŸ”¹ Q: How can I improve my budget?
ðŸ¤– A: Focus on investments spending (â‚¹4,926,868). Try to reduce by 10-15% and increase savings to 25% of income. Recommended SIP: â‚»61,982 monthly. Consider reducing expenses.

ðŸ“„ Improved training summary saved

======================================================================
âœ… IMPROVED TRAINING COMPLETE!
======================================================================
ðŸŽ¯ Model location: ./models/improved_financial_lora
ðŸ“Š Training data: 1600 high-quality examples
â±ï¸ Training time: 2:28:26.590368
ðŸŽ“ Status: READY FOR M.TECH EVALUATION!
======================================================================