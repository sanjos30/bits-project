{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us0BG_LN7zJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71dc37d-2d0d-462b-dc05-edf8d9428b01"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üìÅ Contents of model folder: ['README.md', 'adapter_model.safetensors', 'adapter_config.json', 'special_tokens_map.json', 'tokenizer.model', 'tokenizer.json', 'tokenizer_config.json', 'chat_template.jinja']\n",
            "üì¶ Loading base model...\n",
            "üß© Loading LoRA adapter from: /content/drive/MyDrive/lora-tinyllama-user1\n",
            "üí¨ Finance Copilot ready. Ask your question (type 'exit' to quit):\n",
            "Bot: My total spend on groceries in August 2023 was $1,237.00\n",
            "Bot: I don't have any specific habits or routines that would help you save $1,000 for buying a car. However, here are some general tips that could help:\n",
            "\n",
            "1. Reduce expenses: By reducing expenses, you can save money for buying a car. This could include reducing your daily expenses like eating out or staying in expensive hotels.\n",
            "\n",
            "2. Avoid impulse buying: Avoid impulse buying, especially when it comes to car purchases. If you feel the urge to buy something just because it's new or has good features, it's better to save it for a better time.\n",
            "\n",
            "3. Keep a budget\n",
            "Bot: Your spend summary for year 2023:\n",
            "- Food and Beverage expenses: $2000\n",
            "- Housing expenses: $3000\n",
            "- Transportation expenses: $2500\n",
            "- Other expenses: $1000\n",
            "- Total expenses: $5500\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define model path\n",
        "MODEL_PATH = \"/content/drive/MyDrive/lora-tinyllama-user1\"\n",
        "\n",
        "# Step 2: Check that adapter files exist\n",
        "import os\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"‚ùå Folder not found: {MODEL_PATH}\")\n",
        "else:\n",
        "    contents = os.listdir(MODEL_PATH)\n",
        "    print(\"üìÅ Contents of model folder:\", contents)\n",
        "    required_files = [\"adapter_config.json\", \"adapter_model.safetensors\"]\n",
        "    for f in required_files:\n",
        "        if f not in contents:\n",
        "            raise FileNotFoundError(f\"‚ùå Missing file: {f} in {MODEL_PATH}\")\n",
        "\n",
        "# Step 3: Install libraries\n",
        "!pip install -q peft transformers accelerate bitsandbytes\n",
        "\n",
        "# Step 4: Load model + tokenizer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(\"üì¶ Loading base model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"üß© Loading LoRA adapter from:\", MODEL_PATH)\n",
        "model = PeftModel.from_pretrained(model, MODEL_PATH, is_trainable=False)\n",
        "model.eval()\n",
        "\n",
        "# Step 5: Define chat function\n",
        "def generate_response(user_question):\n",
        "    prompt = f\"### Instruction:\\n{user_question}\\n\\n### Response:\\n\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.9\n",
        "        )\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Only return text after the prompt\n",
        "    return full_output.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "\n",
        "# Step 6: Chat loop\n",
        "print(\"üí¨ Finance Copilot ready. Ask your question (type 'exit' to quit):\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Exiting.\")\n",
        "        break\n",
        "    response = generate_response(user_input)\n",
        "    print(\"Bot:\", response.split(\"###\")[-1].strip())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}